services:
  airflow:
    image: apache/airflow:latest
    container_name: airflow
    ports:
      - "8080:8080"
    environment:
      PYTHONPATH: /opt/airflow/src
      DB_USER: ${DB_USER}
      DB_PASSWORD: ${DB_PASSWORD}
      DB_HOST: ${DB_HOST}
      DB_PORT: ${DB_PORT}
      DB_NAME: ${DB_NAME}
      MINIO_URL: ${MINIO_URL}
      MINIO_USER: ${MINIO_USER}
      MINIO_PASSWORD: ${MINIO_PASSWORD}
    volumes:
      - ./requirements.txt:/requirements.txt
      - ./airflow/airflow.cfg:/opt/airflow/airflow.cfg
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - ./src:/opt/airflow/src
    command: >
      bash -c "
        pip install -r /requirements.txt;
        airflow db init;
        airflow users create --username ${AIRFLOW_USERNAME} --password ${AIRFLOW_PASSWORD} --firstname Admin --lastname User --role Admin --email nomail@nomail.com;
        airflow webserver;
      "
    networks:
      - airflow_network

  airflow-scheduler:
    image: apache/airflow:latest
    container_name: airflow-scheduler
    restart: always
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__CORE__DAGS__FOLDER: /opt/airflow/dags
      AIRFLOW__CORE__LOAD_EXAMPLES: "False"
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${DB_USER}:${DB_PASSWORD}@${DB_HOST}:${DB_PORT}/${DB_NAME}
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://${DB_USER}:${DB_PASSWORD}@${DB_HOST}:${DB_PORT}/${DB_NAME}
      AIRFLOW__CELERY__BROKER_URL: redis://redis:6379/0
      PYTHONPATH: /opt/airflow/src
    depends_on:
      - postgres
      - redis
    volumes:
      - ./requirements.txt:/requirements.txt
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - ./src:/opt/airflow/src
    command: >
      bash -c "
      airflow db upgrade;
      pip install -r /requirements.txt;
      airflow scheduler;
      "
    networks:
      - airflow_network

  airflow-worker:
    image: apache/airflow:latest
    container_name: airflow-worker
    restart: always
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${DB_USER}:${DB_PASSWORD}@${DB_HOST}:${DB_PORT}/${DB_NAME}
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://${DB_USER}:${DB_PASSWORD}@${DB_HOST}:${DB_PORT}/${DB_NAME}
      AIRFLOW__CELERY__BROKER_URL: redis://redis:6379/0
    depends_on:
      - redis
      - postgres
      - airflow-scheduler
    volumes:
      - ./requirements.txt:/requirements.txt
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - ./src:/opt/airflow/src
    command: >
      bash -c "
      pip install -r /requirements.txt;
      airflow celery worker --queues=default;
      "
    networks:
      - airflow_network

  postgres:
    image: postgres
    container_name: postgres
    environment:
      POSTGRES_USER: ${DB_USER}
      POSTGRES_PASSWORD: ${DB_PASSWORD}
      POSTGRES_DB: ${DB_NAME}
    ports:
      - "5432:5432"
    volumes:
      - postgres-data:/var/lib/postgresql/data
    networks:
      - airflow_network

  redis:
    image: redis:6.0
    container_name: redis
    ports:
      - "6379:6379"
    networks:
      - airflow_network

  minio:
    image: quay.io/minio/minio
    container_name: minio
    ports:
      - "9000:9000"
      - "9001:9001"
    environment:
      MINIO_ROOT_USER: ${MINIO_USER}
      MINIO_ROOT_PASSWORD: ${MINIO_PASSWORD}
    command: server /data --console-address ":9001"
    volumes:
      - minio-data:/data
    networks:
      - airflow_network

volumes:
  minio-data:
  postgres-data:

networks:
  airflow_network:
    driver: bridge
